#!/usr/bin/env python3
"""
ä½¿ç”¨ OpenAI çš„ GPT-4o-mini æ¨¡å‹é€šè¿‡ Standard API å’Œ Batch API ç”Ÿæˆé—®é¢˜ã€‚
"""

import os
import time
import json
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from openai import OpenAI
from openai.types import Batch
from openai import APITimeoutError, APIConnectionError, RateLimitError, InternalServerError
import dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv.load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ==================== é…ç½® ====================
MODEL_NAME = "gpt-4o-mini"  # æˆ– "gpt-4o-mini-2024-07-18"
MAX_RETRIES = 3
RETRY_DELAY = 1  # ç§’
BATCH_TIMEOUT_CHECK = 30  # æ£€æŸ¥ Batch çŠ¶æ€çš„é—´éš”ï¼ˆç§’ï¼‰

# ==================== æ•°æ®ç»“æ„ ====================

@dataclass
class QuestionRequest:
    """è¡¨ç¤ºä¸€ä¸ªç”Ÿæˆé—®é¢˜çš„è¯·æ±‚"""
    text: str
    context: str = ""
    system_prompt: str = "You are a helpful assistant that generates insightful, binary (yes/no) questions about the given text."

    def to_openai_format(self, request_id: str) -> Dict[str, Any]:
        """è½¬æ¢ä¸º OpenAI Batch API æ‰€éœ€çš„æ ¼å¼"""
        prompt = f"Text: {self.text}\nContext: {self.context}\n\nGenerate one insightful binary (yes/no) question about this text."
        return {
            "custom_id": request_id,
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": MODEL_NAME,
                "messages": [
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 50,
                "temperature": 0.7
            }
        }

# ==================== æ ¸å¿ƒåŠŸèƒ½ ====================

def generate_question_standard(request: QuestionRequest) -> str:
    """
    ä½¿ç”¨ Standard API å®æ—¶ç”Ÿæˆä¸€ä¸ªé—®é¢˜ï¼ˆåŒæ­¥ï¼‰ã€‚
    
    Args:
        request: é—®é¢˜ç”Ÿæˆè¯·æ±‚
        
    Returns:
        ç”Ÿæˆçš„é—®é¢˜æ–‡æœ¬
    """
    for attempt in range(MAX_RETRIES):
        try:
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": request.system_prompt},
                    {"role": "user", "content": f"Text: {request.text}\nContext: {request.context}\n\nGenerate one insightful binary (yes/no) question about this text."}
                ],
                max_tokens=50,
                temperature=0.7
            )
            # æå–ç”Ÿæˆçš„é—®é¢˜
            question = response.choices[0].message.content.strip()
            logger.info(f"âœ… Standard API: Generated question: {question}")
            return question

        except (APITimeoutError, APIConnectionError) as e:
            logger.warning(f"âš ï¸  Standard API Attempt {attempt + 1} failed: {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY * (2 ** attempt))  # æŒ‡æ•°é€€é¿
            else:
                logger.error("âŒ Standard API: All attempts failed.")
                return "Error: Failed to generate question."
        except Exception as e:
            logger.error(f"âŒ Standard API Unexpected error: {e}")
            return "Error: Unexpected error."

def create_batch_requests(requests: List[QuestionRequest]) -> List[Dict[str, Any]]:
    """
    å°† QuestionRequest åˆ—è¡¨è½¬æ¢ä¸º OpenAI Batch API æ‰€éœ€çš„è¯·æ±‚åˆ—è¡¨ã€‚
    
    Args:
        requests: è¯·æ±‚åˆ—è¡¨
        
    Returns:
        æ ¼å¼åŒ–åçš„è¯·æ±‚åˆ—è¡¨
    """
    batch_requests = []
    for i, req in enumerate(requests):
        batch_req = req.to_openai_format(f"request-{i}")
        batch_requests.append(batch_req)
    return batch_requests

def upload_batch_file(requests_jsonl: str) -> str:
    """
    ä¸Šä¼ åŒ…å«è¯·æ±‚çš„ .jsonl æ–‡ä»¶åˆ° OpenAIã€‚
    
    Args:
        requests_jsonl: .jsonl æ–‡ä»¶çš„è·¯å¾„
        
    Returns:
        ä¸Šä¼ çš„æ–‡ä»¶ ID
    """
    try:
        file = client.files.create(
            file=open(requests_jsonl, "rb"),
            purpose="batch"
        )
        logger.info(f"ğŸ“ Batch File Uploaded. ID: {file.id}")
        return file.id
    except Exception as e:
        logger.error(f"âŒ Failed to upload batch file: {e}")
        raise

def create_and_run_batch(file_id: str) -> Batch:
    """
    åˆ›å»ºå¹¶å¯åŠ¨ä¸€ä¸ª Batch ä»»åŠ¡ã€‚
    
    Args:
        file_id: ä¸Šä¼ çš„æ–‡ä»¶ ID
        
    Returns:
        Batch å¯¹è±¡
    """
    try:
        batch = client.batches.create(
            input_file_id=file_id,
            endpoint="/v1/chat/completions",
            completion_window="24h"
        )
        logger.info(f"ğŸš€ Batch Created. Batch ID: {batch.id}")
        return batch
    except Exception as e:
        logger.error(f"âŒ Failed to create batch: {e}")
        raise

def monitor_batch(batch_id: str) -> Batch:
    """
    ç›‘æ§ Batch çš„çŠ¶æ€ï¼Œç›´åˆ°å®Œæˆæˆ–å¤±è´¥ã€‚
    
    Args:
        batch_id: Batch ID
        
    Returns:
        æœ€ç»ˆçš„ Batch å¯¹è±¡
    """
    while True:
        try:
            batch = client.batches.retrieve(batch_id)
            logger.info(f"ğŸ“Š Batch Status: {batch.status} | "
                        f"Requests: {batch.request_counts.total} "
                        f"(Completed: {batch.request_counts.completed}, "
                        f"Failed: {batch.request_counts.failed})")
            
            if batch.status in ["completed", "failed", "expired", "cancelled"]:
                return batch
            
            time.sleep(BATCH_TIMEOUT_CHECK)
            
        except Exception as e:
            logger.error(f"âŒ Error monitoring batch: {e}")
            time.sleep(BATCH_TIMEOUT_CHECK)

def download_batch_results(batch: Batch) -> List[Dict[str, Any]]:
    """
    ä¸‹è½½ Batch çš„è¾“å‡ºç»“æœã€‚
    
    Args:
        batch: Batch å¯¹è±¡
        
    Returns:
        è§£æåçš„ç»“æœåˆ—è¡¨
    """
    if not batch.output_file_id:
        logger.warning("âš ï¸  No output file for this batch.")
        return []
    
    try:
        # ä¸‹è½½è¾“å‡ºæ–‡ä»¶å†…å®¹
        content = client.files.content(batch.output_file_id)
        output_path = f"batch_output_{batch.id}.jsonl"
        with open(output_path, 'w') as f:
            f.write(content.text)
        logger.info(f"ğŸ“¥ Batch output saved to {output_path}")
        
        # è§£æç»“æœ
        results = []
        with open(output_path, 'r') as f:
            for line in f:
                if line.strip():
                    item = json.loads(line)
                    # æå–é—®é¢˜
                    question = item['response']['body']['choices'][0]['message']['content'].strip()
                    results.append({
                        "id": item['custom_id'],
                        "question": question
                    })
        return results
        
    except Exception as e:
        logger.error(f"âŒ Failed to download or parse batch results: {e}")
        return []

# ==================== ä¸»å‡½æ•°ç¤ºä¾‹ ====================

def main():
    """æ¼”ç¤º Standard API å’Œ Batch API çš„ä½¿ç”¨"""
    
    # ç¤ºä¾‹æ–‡æœ¬åˆ—è¡¨ï¼ˆåœ¨ CQG-MBQA ä¸­ï¼Œè¿™äº›å¯èƒ½æ˜¯èšç±»çš„ä¸­å¿ƒå¥æˆ–ä»£è¡¨æ€§å¥å­ï¼‰
    sample_texts = [
        "The cat sat on the mat.",
        "Machine learning models can learn from data.",
        "Photosynthesis converts sunlight into chemical energy."
    ]
    
    # 1. ä½¿ç”¨ Standard API ç”Ÿæˆé—®é¢˜ï¼ˆå®æ—¶ï¼‰
    print("\n" + "="*60)
    print("ğŸŸ¢ USING STANDARD API (Real-time)")
    print("="*60)
    
    standard_questions = []
    for text in sample_texts:
        req = QuestionRequest(text=text, context="General knowledge.")
        question = generate_question_standard(req)
        standard_questions.append(question)
    
    print("\nGenerated Questions (Standard API):")
    for q in standard_questions:
        print(f"  - {q}")
    
    # 2. ä½¿ç”¨ Batch API ç”Ÿæˆé—®é¢˜ï¼ˆå¼‚æ­¥ï¼‰
    print("\n" + "="*60)
    print("ğŸ”µ USING BATCH API (Asynchronous)")
    print("="*60)
    
    # åˆ›å»ºè¯·æ±‚
    batch_requests = [QuestionRequest(text=text, context="General knowledge.") for text in sample_texts]
    formatted_requests = create_batch_requests(batch_requests)
    
    # ä¿å­˜ä¸º .jsonl æ–‡ä»¶
    input_file = "batch_input.jsonl"
    with open(input_file, 'w') as f:
        for req in formatted_requests:
            f.write(json.dumps(req) + '\n')
    logger.info(f"ğŸ“„ Batch input saved to {input_file}")
    
    # ä¸Šä¼ æ–‡ä»¶
    file_id = upload_batch_file(input_file)
    
    # åˆ›å»ºå¹¶è¿è¡Œ Batch
    batch = create_and_run_batch(file_id)
    
    # ç›‘æ§ Batch çŠ¶æ€
    print("\nâ³ Monitoring batch status... (This may take a few minutes)")
    final_batch = monitor_batch(batch.id)
    
    if final_batch.status == "completed":
        # ä¸‹è½½å¹¶è§£æç»“æœ
        batch_results = download_batch_results(final_batch)
        print(f"\nâœ… Batch completed! Retrieved {len(batch_results)} results.")
        
        print("\nGenerated Questions (Batch API):")
        for result in batch_results:
            print(f"  - {result['question']}")
    else:
        logger.error(f"âŒ Batch failed with status: {final_batch.status}")

if __name__ == "__main__":
    main()